# -*- coding: utf-8 -*-
"""DAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vHcAMKFbzTPPW7gR154NS2Hqe1SODOB0
"""

# from google.colab import drive
import pandas as pd
import numpy as np
from scipy import sparse
# import os
# drive.mount('/content/drive', force_remount=True)

data = pd.read_csv('./ratings.csv', header=0)

# Filter out ratings < 4
data = data[data["rating"]>=4]

# Remove users with less than 5 interactions
user_dist = data[["userId"]].groupby("userId",as_index=False).size()
data = data[data['userId'].isin(user_dist[user_dist >= 5].index)]

# Get movie and user distribution
movie_dist = data[["movieId"]].groupby("movieId",as_index=False).size()
user_dist = data[["userId"]].groupby("userId",as_index=False).size()

# Split data
user_dist = user_dist.sample(frac=1)
user_list = user_dist.index

user_list_tr = user_list[:-20000]
user_list_te = user_list[-20000:-10000]
user_list_vd = user_list[-10000:]

data_tr = data[data['userId'].isin(user_list_tr)]
unique_movie_tr = pd.unique(data_tr['movieId'])
movie2ind_tr = dict((ind, i) for (i, ind) in enumerate(unique_movie_tr))
user2ind = dict((ind, i) for (i, ind) in enumerate(user_list))

# Save the lists
# data_root = '/content/drive/My Drive/MDM/ml-20/'
# save_dir = data_root + 'lists'
# if not os.path.exists(save_dir):
#   os.mkdir(save_dir)
# with open(os.path.join(save_dir, 'unique_movie_tr.txt'), 'w') as f:
#     for movie in unique_movie_tr:
#         f.write('%s\n' % movie)

def numerize(dt):
    uid = list(map(lambda x: user2ind[x], dt['userId']))
    sid = list(map(lambda x: movie2ind_tr[x], dt['movieId']))
    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])

train_data = numerize(data_tr)
# train_data.to_csv(os.path.join(save_dir, 'train.csv'), index=False)

num_movies = len(unique_movie_tr)
num_users = data_tr['userId'].max() + 1
rows, cols = train_data['uid'], train_data['sid']

# Data has entry 1 where the user has rated the movie: highly sparse matrix
training_data = sparse.csr_matrix((np.ones_like(rows),(rows, cols)), dtype='float64',shape=(num_users, num_movies))

def split_train_test_proportion(data, test_prop=0.2):
    data_grouped_by_user = data.groupby('userId')
    tr_list, te_list = list(), list()

    np.random.seed(98765)

    for i, (_, group) in enumerate(data_grouped_by_user):
        n_items_u = len(group)

        if n_items_u >= 5:
            idx = np.zeros(n_items_u, dtype='bool')
            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True

            tr_list.append(group[np.logical_not(idx)])
            te_list.append(group[idx])
        else:
            tr_list.append(group) 

    data_tr = pd.concat(tr_list)
    data_te = pd.concat(te_list)
    
    return data_tr, data_te

data_test = data.loc[data['userId'].isin(user_list_te)]
data_test = data_test.loc[data['movieId'].isin(unique_movie_tr)]
data_test_tr, data_test_te = split_train_test_proportion(data_test)

data_test_tr = numerize(data_test_tr)
data_test_te = numerize(data_test_te)

# data_test_tr.to_csv(os.path.join(save_dir, 'data_test_tr.csv'), index=False)
# data_test_te.to_csv(os.path.join(save_dir, 'data_test_te.csv'), index=False)

data_vd = data.loc[data['userId'].isin(user_list_vd)]
data_vd = data_vd.loc[data['movieId'].isin(unique_movie_tr)]
data_vd_tr, data_vd_te = split_train_test_proportion(data_vd)

data_vd_tr = numerize(data_vd_tr)
data_vd_te = numerize(data_vd_te)

# data_vd_tr.to_csv(os.path.join(save_dir, 'data_vd_tr.csv'), index=False)
# data_vd_te.to_csv(os.path.join(save_dir, 'data_vd_te.csv'), index=False)

import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout
from tensorflow.keras import Model
from tensorflow import Variable 
from tensorflow.keras import initializers


class MyModel(Model):
  def __init__(self, input_features, inherent_features ,weight_decay=0.0, drop_prob=0):
    super(MyModel, self).__init__()
    self.weight_decay_rate = weight_decay
    self.tanh = Dense(inherent_features, activation='tanh',kernel_initializer=tf.keras.initializers.GlorotNormal(), bias_initializer = tf.keras.initializers.TruncatedNormal(stddev=0.001) )
    self.linear_layer = Dense(input_features, activation='linear', kernel_initializer=tf.keras.initializers.GlorotNormal(), bias_initializer = tf.keras.initializers.TruncatedNormal(stddev=0.001))
    self.dropout = Dropout(drop_prob)
    # self.normalization_layer = tf.math.l2_normalize(axis=1, epsilon=1e-12)
    # self.log_softmax = tf.nn.log_softmax(axis=-1)
    # self.tanh = Dense(activation='tanh')

  def call(self, x):
    normalized_output = tf.math.l2_normalize(x, axis=1, epsilon=1e-12) #default 2nd degree normalizer
    dropout_output = self.dropout(normalized_output)
    tanh = self.tanh(dropout_output)
    dense_output = self.linear_layer(tanh)
    return dense_output

  def loss(self, pred, target):
    pred = tf.nn.log_softmax(pred , axis=-1)
    loss_matrix = pred*target
    loss = -tf.math.reduce_mean(loss_matrix, axis=1)
    return tf.math.reduce_mean(loss, axis=0)
  
  def l2_reg(self):
    # l2_reg = Variable(0)
    # for l in [self.layers[0]]:
    #   temp = tf.norm(l.weights, ord =2, axis=1) ** 2
    #   # temp =  tf.math.l2_normalize(l.weights, axis=1, epsilon=1e-12)
    #   print(temp.shape)
    #   input()
    #   l2_reg = l2_reg + temp
    #   l2_reg = self.weight_decay_rate * l2_reg
    # return l2_reg[0]

    regularizer = tf.keras.regularizers.L2(0.01)
    l2_reg = 0
    for l in self.layers:      
      if(len(l.weights)>0):
        l2_reg += regularizer(l.weights[0])*self.weight_decay_rate
    return l2_reg

N = training_data.shape[0]
# idxlist = range(N)
batch_size = 500
batches_per_epoch = int(np.ceil(float(N) / batch_size))

def load_tr_te_data(tp_tr, tp_te):
    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())
    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())

    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']
    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']

    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),
                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, num_movies))
    data_te = sparse.csr_matrix((np.ones_like(rows_te),
                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, num_movies))
    return data_tr, data_te

vad_data_tr, vad_data_te = load_tr_te_data(data_vd_tr,data_vd_te)

import bottleneck as bn
def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):
    '''
    normalized discounted cumulative gain@k for binary relevance
    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance
    '''
    batch_users = X_pred.shape[0]
    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)
    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],
                       idx_topk_part[:, :k]]
    idx_part = np.argsort(-topk_part, axis=1)
    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted
    # topk predicted score
    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]
    # build the discount template
    tp = 1. / np.log2(np.arange(2, k + 2))

    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],
                         idx_topk].toarray() * tp).sum(axis=1)
    IDCG = np.array([(tp[:min(n, k)]).sum()
                     for n in heldout_batch.getnnz(axis=1)])
    return DCG / IDCG

# weight initialization
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        tf.keras.initializer.GlorotNormal(m.weight.data) ## might throw error
        tf.keras.initializer.RandomNormal(m.bias.data,stddev=0.001)
        m.bias.data.clamp_(-2*0.001, 2*0.001) # temporary fix: this will set the values to 0 but 
        # in tf the values are redrawn : write a function for this if necessary

#call model
device = tf.device('cuda' if tf.test.is_gpu_available(cuda_only=True) else 'cpu')
model = MyModel(num_movies,200,weight_decay=0.01/500)
# model.apply(weights_init)
optimizer =tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.0)

# # #define default pytorch dataloader
# # # from torch.utils import data
# # # assumes 'training_data' is np array
# # # need to chage the structure of array from [a,b] to [a,[b,1]]
# # #training_data= sparse.csr_matrix.toarray(training_data)
# # # dataloader= torch.utils.data.DataLoader(training_data,batch_size = 500,drop_last=True)

from scipy import sparse

def convert_to_sparse_matrix(x):
  indices = [[]]
  values = []
  finalShape = x.shape
  rows, cols = x.nonzero()
  for row, col in zip(rows, cols):
    indices.append((row, col))
    values.append(x[row,col])
  return tf.sparse.SparseTensor(indices[1:], values, finalShape)

# print(training_data.shape)
sparse_training_data = convert_to_sparse_matrix(training_data)
dataloader = tf.data.Dataset.from_tensor_slices(sparse_training_data)

dataloader =  dataloader.batch(500, drop_remainder=True)

#for validation data
batch_size_vd = batch_size
no_batches = 10000/batch_size_vd
batch_list_vd = np.arange((no_batches)).astype('int32')
rng = np.random.default_rng()
rng.shuffle(batch_list_vd)

# loop for training
# training_data

# batches_per_epoch = 138494/500
batch_list = np.arange((batches_per_epoch-2))

rng = np.random.default_rng()
rng.shuffle(batch_list)

ndcg_vad =[]
# from torch.autograd import Variable
ndcgs_vad=[]
out_val = []
for epoch in range(25):
  rng.shuffle(batch_list)
  i = 0
  loss = 0
  loss_total = 0
  while i < len(batch_list):
    data = training_data[batch_list[i]*batch_size:(batch_list[i]+1)*batch_size].toarray()
    i+=1
    # model.zero_grad() ## try this
    with tf.GradientTape() as tape:
      inputs=tf.convert_to_tensor(data, dtype='float32')
      inputv = Variable(inputs)
      pred = model(inputv)
      loss = model.loss(pred,inputv)
      los_reg = model.l2_reg()
      loss_total = loss+los_reg
    gradients = tape.gradient(loss_total, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  print(epoch, loss)

  ndcg_dist = []
  j=0
  while j< len(batch_list_vd):
    
    
    data_vd_tr = vad_data_tr[batch_list_vd[j]*batch_size:(batch_list_vd[j]+1)*batch_size].toarray()
    data_vd_te = vad_data_te[batch_list_vd[j]*batch_size:(batch_list_vd[j]+1)*batch_size]#.toarray()
    pred_val = model(Variable(tf.convert_to_tensor(data_vd_tr)))
    pred_val = pred_val.cpu().numpy()
    pred_val[data_vd_tr.nonzero()] = -np.inf
    ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, data_vd_te))
    j+=1
  ndcg = np.nan_to_num(ndcg_dist, nan=1)
  ndcg_dist1 = np.concatenate(ndcg) 
  #   print(np.mean(ndcg_dist1))
  out_val.append(np.mean(ndcg_dist1))
  print("-----------------------")
  print(out_val)
  print("-----------------------")

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
x = np.arange(1,3,1)

plt.figure(figsize=(12, 3))
plt.plot(x,ndcg_vad,'r-',label='final')
plt.ylabel("Validation NDCG@100")
plt.xlabel("Epochs")
plt.legend()
plt.savefig(save_dir+'/combine')

